{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553bd2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.optimize as spo\n",
    "import pandas as pd\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a38a478",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 2s 36ms/step - loss: 0.8152 - accuracy: 0.4880 - val_loss: 0.8020 - val_accuracy: 0.4960\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.8040 - accuracy: 0.4900 - val_loss: 0.7915 - val_accuracy: 0.5000\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.7930 - accuracy: 0.4880 - val_loss: 0.7811 - val_accuracy: 0.4980\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.7826 - accuracy: 0.4839 - val_loss: 0.7716 - val_accuracy: 0.4980\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.7733 - accuracy: 0.4859 - val_loss: 0.7616 - val_accuracy: 0.4980\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.7632 - accuracy: 0.4880 - val_loss: 0.7537 - val_accuracy: 0.5000\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.7548 - accuracy: 0.4880 - val_loss: 0.7451 - val_accuracy: 0.5000\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.7465 - accuracy: 0.4920 - val_loss: 0.7358 - val_accuracy: 0.4960\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.7368 - accuracy: 0.4920 - val_loss: 0.7269 - val_accuracy: 0.5020\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.7277 - accuracy: 0.4920 - val_loss: 0.7169 - val_accuracy: 0.5020\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.7185 - accuracy: 0.4960 - val_loss: 0.7084 - val_accuracy: 0.5120\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.7116 - accuracy: 0.4940 - val_loss: 0.7024 - val_accuracy: 0.5141\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.7060 - accuracy: 0.4940 - val_loss: 0.6992 - val_accuracy: 0.5120\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.7028 - accuracy: 0.4900 - val_loss: 0.6964 - val_accuracy: 0.5100\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.7001 - accuracy: 0.4859 - val_loss: 0.6952 - val_accuracy: 0.5080\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6985 - accuracy: 0.4839 - val_loss: 0.6943 - val_accuracy: 0.5100\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6972 - accuracy: 0.4880 - val_loss: 0.6937 - val_accuracy: 0.5100\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6966 - accuracy: 0.5020 - val_loss: 0.6935 - val_accuracy: 0.5120\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6959 - accuracy: 0.5100 - val_loss: 0.6941 - val_accuracy: 0.4880\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6951 - accuracy: 0.5141 - val_loss: 0.6935 - val_accuracy: 0.4900\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6946 - accuracy: 0.5060 - val_loss: 0.6937 - val_accuracy: 0.4839\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6940 - accuracy: 0.5221 - val_loss: 0.6939 - val_accuracy: 0.4819\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6936 - accuracy: 0.5201 - val_loss: 0.6941 - val_accuracy: 0.4859\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.5181 - val_loss: 0.6941 - val_accuracy: 0.4880\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.5141 - val_loss: 0.6952 - val_accuracy: 0.4940\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6927 - accuracy: 0.5221 - val_loss: 0.6937 - val_accuracy: 0.4980\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6926 - accuracy: 0.5161 - val_loss: 0.6937 - val_accuracy: 0.5020\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6921 - accuracy: 0.5161 - val_loss: 0.6941 - val_accuracy: 0.4940\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6921 - accuracy: 0.5040 - val_loss: 0.6949 - val_accuracy: 0.5000\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6920 - accuracy: 0.4960 - val_loss: 0.6941 - val_accuracy: 0.4960\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6917 - accuracy: 0.5000 - val_loss: 0.6942 - val_accuracy: 0.5120\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6916 - accuracy: 0.5000 - val_loss: 0.6942 - val_accuracy: 0.5141\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6918 - accuracy: 0.5040 - val_loss: 0.6944 - val_accuracy: 0.4940\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6916 - accuracy: 0.5201 - val_loss: 0.6945 - val_accuracy: 0.5000\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6915 - accuracy: 0.5301 - val_loss: 0.6950 - val_accuracy: 0.5261\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6913 - accuracy: 0.5341 - val_loss: 0.6950 - val_accuracy: 0.5241\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6915 - accuracy: 0.5040 - val_loss: 0.6952 - val_accuracy: 0.5281\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6913 - accuracy: 0.5241 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6910 - accuracy: 0.5221 - val_loss: 0.6942 - val_accuracy: 0.5000\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6910 - accuracy: 0.5120 - val_loss: 0.6942 - val_accuracy: 0.4839\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6908 - accuracy: 0.5161 - val_loss: 0.6947 - val_accuracy: 0.4819\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6912 - accuracy: 0.5201 - val_loss: 0.6951 - val_accuracy: 0.4819\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6907 - accuracy: 0.5241 - val_loss: 0.6943 - val_accuracy: 0.4699\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6906 - accuracy: 0.5161 - val_loss: 0.6944 - val_accuracy: 0.4679\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6906 - accuracy: 0.5221 - val_loss: 0.6948 - val_accuracy: 0.4659\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6906 - accuracy: 0.5241 - val_loss: 0.6956 - val_accuracy: 0.4859\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6903 - accuracy: 0.5261 - val_loss: 0.6945 - val_accuracy: 0.4699\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6903 - accuracy: 0.5261 - val_loss: 0.6945 - val_accuracy: 0.4779\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6903 - accuracy: 0.5261 - val_loss: 0.6946 - val_accuracy: 0.4779\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6900 - accuracy: 0.5221 - val_loss: 0.6951 - val_accuracy: 0.4679\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6900 - accuracy: 0.5281 - val_loss: 0.6956 - val_accuracy: 0.4739\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6902 - accuracy: 0.5161 - val_loss: 0.6951 - val_accuracy: 0.4719\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6901 - accuracy: 0.5141 - val_loss: 0.6962 - val_accuracy: 0.4659\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6900 - accuracy: 0.5301 - val_loss: 0.6958 - val_accuracy: 0.4779\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6896 - accuracy: 0.5281 - val_loss: 0.6954 - val_accuracy: 0.4739\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6900 - accuracy: 0.5201 - val_loss: 0.6954 - val_accuracy: 0.4719\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6899 - accuracy: 0.5161 - val_loss: 0.6960 - val_accuracy: 0.4779\n",
      "Epoch 58/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6896 - accuracy: 0.5221 - val_loss: 0.6960 - val_accuracy: 0.4739\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6894 - accuracy: 0.5241 - val_loss: 0.6957 - val_accuracy: 0.4739\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6893 - accuracy: 0.5321 - val_loss: 0.6958 - val_accuracy: 0.4719\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6892 - accuracy: 0.5281 - val_loss: 0.6962 - val_accuracy: 0.4659\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6894 - accuracy: 0.5241 - val_loss: 0.6962 - val_accuracy: 0.4578\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6891 - accuracy: 0.5241 - val_loss: 0.6961 - val_accuracy: 0.4679\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6892 - accuracy: 0.5201 - val_loss: 0.6964 - val_accuracy: 0.4739\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6893 - accuracy: 0.5281 - val_loss: 0.6965 - val_accuracy: 0.4558\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6889 - accuracy: 0.5221 - val_loss: 0.6960 - val_accuracy: 0.4639\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6888 - accuracy: 0.5361 - val_loss: 0.6962 - val_accuracy: 0.4639\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6886 - accuracy: 0.5281 - val_loss: 0.6966 - val_accuracy: 0.4598\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6888 - accuracy: 0.5341 - val_loss: 0.6974 - val_accuracy: 0.4659\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6886 - accuracy: 0.5301 - val_loss: 0.6966 - val_accuracy: 0.4578\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6886 - accuracy: 0.5221 - val_loss: 0.6968 - val_accuracy: 0.4598\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6883 - accuracy: 0.5201 - val_loss: 0.6968 - val_accuracy: 0.4618\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6884 - accuracy: 0.5281 - val_loss: 0.6969 - val_accuracy: 0.4779\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6883 - accuracy: 0.5321 - val_loss: 0.6970 - val_accuracy: 0.4699\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6881 - accuracy: 0.5382 - val_loss: 0.6970 - val_accuracy: 0.4719\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6882 - accuracy: 0.5341 - val_loss: 0.6969 - val_accuracy: 0.4819\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6885 - accuracy: 0.5141 - val_loss: 0.6974 - val_accuracy: 0.4819\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6880 - accuracy: 0.5281 - val_loss: 0.6971 - val_accuracy: 0.4859\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6880 - accuracy: 0.5341 - val_loss: 0.6980 - val_accuracy: 0.4739\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6877 - accuracy: 0.5261 - val_loss: 0.6975 - val_accuracy: 0.4719\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6876 - accuracy: 0.5201 - val_loss: 0.6978 - val_accuracy: 0.4819\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6875 - accuracy: 0.5382 - val_loss: 0.6971 - val_accuracy: 0.4759\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6877 - accuracy: 0.5402 - val_loss: 0.6972 - val_accuracy: 0.4960\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6874 - accuracy: 0.5321 - val_loss: 0.6980 - val_accuracy: 0.4779\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6873 - accuracy: 0.5382 - val_loss: 0.6977 - val_accuracy: 0.4819\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6870 - accuracy: 0.5382 - val_loss: 0.6978 - val_accuracy: 0.4779\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6873 - accuracy: 0.5281 - val_loss: 0.6986 - val_accuracy: 0.4639\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6877 - accuracy: 0.5341 - val_loss: 0.6974 - val_accuracy: 0.4859\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6869 - accuracy: 0.5502 - val_loss: 0.6982 - val_accuracy: 0.4759\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6873 - accuracy: 0.5241 - val_loss: 0.6989 - val_accuracy: 0.4699\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6866 - accuracy: 0.5422 - val_loss: 0.6983 - val_accuracy: 0.4940\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6867 - accuracy: 0.5482 - val_loss: 0.6984 - val_accuracy: 0.4839\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6872 - accuracy: 0.5281 - val_loss: 0.6997 - val_accuracy: 0.4779\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6866 - accuracy: 0.5321 - val_loss: 0.6990 - val_accuracy: 0.4719\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6861 - accuracy: 0.5301 - val_loss: 0.6991 - val_accuracy: 0.4759\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6863 - accuracy: 0.5382 - val_loss: 0.6989 - val_accuracy: 0.4859\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6862 - accuracy: 0.5422 - val_loss: 0.6999 - val_accuracy: 0.4699\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6863 - accuracy: 0.5261 - val_loss: 0.6997 - val_accuracy: 0.4699\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6863 - accuracy: 0.5341 - val_loss: 0.6989 - val_accuracy: 0.4920\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6857 - accuracy: 0.5462 - val_loss: 0.6993 - val_accuracy: 0.4940\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6856 - accuracy: 0.5301 - val_loss: 0.7007 - val_accuracy: 0.4659\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6855 - accuracy: 0.5341 - val_loss: 0.7002 - val_accuracy: 0.4779\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6855 - accuracy: 0.5502 - val_loss: 0.7000 - val_accuracy: 0.4880\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6855 - accuracy: 0.5281 - val_loss: 0.7006 - val_accuracy: 0.4719\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6856 - accuracy: 0.5341 - val_loss: 0.7002 - val_accuracy: 0.4859\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6854 - accuracy: 0.5422 - val_loss: 0.7010 - val_accuracy: 0.4659\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6854 - accuracy: 0.5462 - val_loss: 0.7000 - val_accuracy: 0.4839\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6850 - accuracy: 0.5422 - val_loss: 0.7008 - val_accuracy: 0.4779\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6850 - accuracy: 0.5361 - val_loss: 0.7014 - val_accuracy: 0.4799\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6846 - accuracy: 0.5462 - val_loss: 0.7008 - val_accuracy: 0.4859\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6851 - accuracy: 0.5382 - val_loss: 0.7014 - val_accuracy: 0.4839\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6845 - accuracy: 0.5341 - val_loss: 0.7013 - val_accuracy: 0.4779\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6845 - accuracy: 0.5502 - val_loss: 0.7013 - val_accuracy: 0.4779\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6847 - accuracy: 0.5542 - val_loss: 0.7011 - val_accuracy: 0.4839\n",
      "Epoch 115/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6845 - accuracy: 0.5422 - val_loss: 0.7025 - val_accuracy: 0.4719\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6843 - accuracy: 0.5442 - val_loss: 0.7023 - val_accuracy: 0.4779\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6841 - accuracy: 0.5482 - val_loss: 0.7027 - val_accuracy: 0.4819\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6839 - accuracy: 0.5622 - val_loss: 0.7019 - val_accuracy: 0.4880\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6839 - accuracy: 0.5241 - val_loss: 0.7026 - val_accuracy: 0.4859\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6837 - accuracy: 0.5321 - val_loss: 0.7027 - val_accuracy: 0.4759\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6837 - accuracy: 0.5522 - val_loss: 0.7026 - val_accuracy: 0.4779\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6834 - accuracy: 0.5502 - val_loss: 0.7028 - val_accuracy: 0.4759\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6835 - accuracy: 0.5301 - val_loss: 0.7025 - val_accuracy: 0.4880\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6834 - accuracy: 0.5241 - val_loss: 0.7034 - val_accuracy: 0.4859\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6833 - accuracy: 0.5341 - val_loss: 0.7034 - val_accuracy: 0.4799\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6830 - accuracy: 0.5462 - val_loss: 0.7036 - val_accuracy: 0.4859\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6829 - accuracy: 0.5261 - val_loss: 0.7043 - val_accuracy: 0.4940\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6829 - accuracy: 0.5141 - val_loss: 0.7042 - val_accuracy: 0.4779\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6829 - accuracy: 0.5301 - val_loss: 0.7046 - val_accuracy: 0.4839\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6828 - accuracy: 0.5100 - val_loss: 0.7038 - val_accuracy: 0.4880\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6828 - accuracy: 0.5261 - val_loss: 0.7048 - val_accuracy: 0.4839\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6823 - accuracy: 0.5321 - val_loss: 0.7045 - val_accuracy: 0.4819\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6827 - accuracy: 0.5281 - val_loss: 0.7057 - val_accuracy: 0.4859\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6824 - accuracy: 0.5361 - val_loss: 0.7049 - val_accuracy: 0.4759\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6822 - accuracy: 0.5241 - val_loss: 0.7053 - val_accuracy: 0.4779\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6820 - accuracy: 0.5141 - val_loss: 0.7062 - val_accuracy: 0.4880\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6821 - accuracy: 0.5181 - val_loss: 0.7062 - val_accuracy: 0.4759\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6819 - accuracy: 0.5241 - val_loss: 0.7059 - val_accuracy: 0.4819\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6817 - accuracy: 0.5321 - val_loss: 0.7063 - val_accuracy: 0.4819\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6816 - accuracy: 0.5301 - val_loss: 0.7066 - val_accuracy: 0.4799\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6815 - accuracy: 0.5221 - val_loss: 0.7069 - val_accuracy: 0.4719\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6813 - accuracy: 0.5221 - val_loss: 0.7075 - val_accuracy: 0.4659\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6814 - accuracy: 0.5221 - val_loss: 0.7079 - val_accuracy: 0.4819\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6812 - accuracy: 0.5221 - val_loss: 0.7074 - val_accuracy: 0.4819\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6812 - accuracy: 0.5201 - val_loss: 0.7075 - val_accuracy: 0.4819\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6810 - accuracy: 0.5301 - val_loss: 0.7074 - val_accuracy: 0.4578\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6809 - accuracy: 0.5221 - val_loss: 0.7080 - val_accuracy: 0.4558\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6810 - accuracy: 0.5141 - val_loss: 0.7080 - val_accuracy: 0.4679\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6807 - accuracy: 0.5221 - val_loss: 0.7081 - val_accuracy: 0.4578\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6808 - accuracy: 0.5321 - val_loss: 0.7080 - val_accuracy: 0.4598\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6805 - accuracy: 0.5321 - val_loss: 0.7085 - val_accuracy: 0.4618\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6805 - accuracy: 0.5341 - val_loss: 0.7092 - val_accuracy: 0.4618\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6809 - accuracy: 0.5542 - val_loss: 0.7086 - val_accuracy: 0.4578\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6803 - accuracy: 0.5442 - val_loss: 0.7089 - val_accuracy: 0.4639\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6809 - accuracy: 0.5281 - val_loss: 0.7083 - val_accuracy: 0.4639\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6803 - accuracy: 0.5382 - val_loss: 0.7098 - val_accuracy: 0.4618\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6799 - accuracy: 0.5382 - val_loss: 0.7103 - val_accuracy: 0.4598\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6799 - accuracy: 0.5321 - val_loss: 0.7094 - val_accuracy: 0.4639\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6798 - accuracy: 0.5382 - val_loss: 0.7098 - val_accuracy: 0.4618\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6798 - accuracy: 0.5402 - val_loss: 0.7107 - val_accuracy: 0.4618\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6797 - accuracy: 0.5402 - val_loss: 0.7109 - val_accuracy: 0.4598\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6796 - accuracy: 0.5382 - val_loss: 0.7110 - val_accuracy: 0.4639\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6795 - accuracy: 0.5542 - val_loss: 0.7109 - val_accuracy: 0.4518\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6793 - accuracy: 0.5462 - val_loss: 0.7113 - val_accuracy: 0.4639\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6792 - accuracy: 0.5542 - val_loss: 0.7115 - val_accuracy: 0.4659\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6796 - accuracy: 0.5562 - val_loss: 0.7119 - val_accuracy: 0.4598\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6792 - accuracy: 0.5522 - val_loss: 0.7108 - val_accuracy: 0.4598\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6791 - accuracy: 0.5442 - val_loss: 0.7117 - val_accuracy: 0.4558\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6789 - accuracy: 0.5502 - val_loss: 0.7119 - val_accuracy: 0.4578\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6788 - accuracy: 0.5382 - val_loss: 0.7120 - val_accuracy: 0.4659\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6790 - accuracy: 0.5301 - val_loss: 0.7111 - val_accuracy: 0.4598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6787 - accuracy: 0.5462 - val_loss: 0.7120 - val_accuracy: 0.4659\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6793 - accuracy: 0.5542 - val_loss: 0.7134 - val_accuracy: 0.4538\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6786 - accuracy: 0.5502 - val_loss: 0.7118 - val_accuracy: 0.4558\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6783 - accuracy: 0.5542 - val_loss: 0.7125 - val_accuracy: 0.4659\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6783 - accuracy: 0.5582 - val_loss: 0.7124 - val_accuracy: 0.4558\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6781 - accuracy: 0.5542 - val_loss: 0.7133 - val_accuracy: 0.4538\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6781 - accuracy: 0.5542 - val_loss: 0.7127 - val_accuracy: 0.4538\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6783 - accuracy: 0.5582 - val_loss: 0.7133 - val_accuracy: 0.4618\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6778 - accuracy: 0.5482 - val_loss: 0.7141 - val_accuracy: 0.4498\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6777 - accuracy: 0.5502 - val_loss: 0.7142 - val_accuracy: 0.4558\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6781 - accuracy: 0.5582 - val_loss: 0.7150 - val_accuracy: 0.4418\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6779 - accuracy: 0.5602 - val_loss: 0.7147 - val_accuracy: 0.4578\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6774 - accuracy: 0.5542 - val_loss: 0.7144 - val_accuracy: 0.4679\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6772 - accuracy: 0.5542 - val_loss: 0.7143 - val_accuracy: 0.4578\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6774 - accuracy: 0.5522 - val_loss: 0.7148 - val_accuracy: 0.4598\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6773 - accuracy: 0.5562 - val_loss: 0.7157 - val_accuracy: 0.4518\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6771 - accuracy: 0.5562 - val_loss: 0.7152 - val_accuracy: 0.4639\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6770 - accuracy: 0.5502 - val_loss: 0.7151 - val_accuracy: 0.4518\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6770 - accuracy: 0.5482 - val_loss: 0.7162 - val_accuracy: 0.4458\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6768 - accuracy: 0.5502 - val_loss: 0.7156 - val_accuracy: 0.4618\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6766 - accuracy: 0.5542 - val_loss: 0.7159 - val_accuracy: 0.4639\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6765 - accuracy: 0.5522 - val_loss: 0.7161 - val_accuracy: 0.4478\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6766 - accuracy: 0.5522 - val_loss: 0.7159 - val_accuracy: 0.4518\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6763 - accuracy: 0.5522 - val_loss: 0.7167 - val_accuracy: 0.4518\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6768 - accuracy: 0.5522 - val_loss: 0.7169 - val_accuracy: 0.4498\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6766 - accuracy: 0.5542 - val_loss: 0.7162 - val_accuracy: 0.4578\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6761 - accuracy: 0.5522 - val_loss: 0.7169 - val_accuracy: 0.4598\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6762 - accuracy: 0.5562 - val_loss: 0.7179 - val_accuracy: 0.4438\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6760 - accuracy: 0.5542 - val_loss: 0.7173 - val_accuracy: 0.4578\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6759 - accuracy: 0.5582 - val_loss: 0.7169 - val_accuracy: 0.4618\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6760 - accuracy: 0.5562 - val_loss: 0.7173 - val_accuracy: 0.4578\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6764 - accuracy: 0.5622 - val_loss: 0.7166 - val_accuracy: 0.4558\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6755 - accuracy: 0.5522 - val_loss: 0.7186 - val_accuracy: 0.4498\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6758 - accuracy: 0.5602 - val_loss: 0.7191 - val_accuracy: 0.4498\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6753 - accuracy: 0.5582 - val_loss: 0.7177 - val_accuracy: 0.4538\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6751 - accuracy: 0.5582 - val_loss: 0.7174 - val_accuracy: 0.4598\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6750 - accuracy: 0.5622 - val_loss: 0.7180 - val_accuracy: 0.4618\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6750 - accuracy: 0.5743 - val_loss: 0.7193 - val_accuracy: 0.4538\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6752 - accuracy: 0.5683 - val_loss: 0.7181 - val_accuracy: 0.4558\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6750 - accuracy: 0.5663 - val_loss: 0.7195 - val_accuracy: 0.4498\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6747 - accuracy: 0.5703 - val_loss: 0.7182 - val_accuracy: 0.4639\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6745 - accuracy: 0.5703 - val_loss: 0.7184 - val_accuracy: 0.4659\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6743 - accuracy: 0.5663 - val_loss: 0.7189 - val_accuracy: 0.4598\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6745 - accuracy: 0.5643 - val_loss: 0.7189 - val_accuracy: 0.4639\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6743 - accuracy: 0.5643 - val_loss: 0.7186 - val_accuracy: 0.4639\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6738 - accuracy: 0.5743 - val_loss: 0.7197 - val_accuracy: 0.4618\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6744 - accuracy: 0.5562 - val_loss: 0.7212 - val_accuracy: 0.4578\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6737 - accuracy: 0.5703 - val_loss: 0.7197 - val_accuracy: 0.4598\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6736 - accuracy: 0.5723 - val_loss: 0.7195 - val_accuracy: 0.4699\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6738 - accuracy: 0.5723 - val_loss: 0.7197 - val_accuracy: 0.4699\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6738 - accuracy: 0.5683 - val_loss: 0.7192 - val_accuracy: 0.4659\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6743 - accuracy: 0.5643 - val_loss: 0.7205 - val_accuracy: 0.4659\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6732 - accuracy: 0.5683 - val_loss: 0.7195 - val_accuracy: 0.4659\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6732 - accuracy: 0.5703 - val_loss: 0.7201 - val_accuracy: 0.4639\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6729 - accuracy: 0.5663 - val_loss: 0.7202 - val_accuracy: 0.4699\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6729 - accuracy: 0.5763 - val_loss: 0.7205 - val_accuracy: 0.4679\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6730 - accuracy: 0.5723 - val_loss: 0.7194 - val_accuracy: 0.4639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6727 - accuracy: 0.5743 - val_loss: 0.7202 - val_accuracy: 0.4659\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6726 - accuracy: 0.5823 - val_loss: 0.7206 - val_accuracy: 0.4618\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6725 - accuracy: 0.5703 - val_loss: 0.7215 - val_accuracy: 0.4659\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6726 - accuracy: 0.5703 - val_loss: 0.7212 - val_accuracy: 0.4639\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6726 - accuracy: 0.5723 - val_loss: 0.7217 - val_accuracy: 0.4679\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6723 - accuracy: 0.5743 - val_loss: 0.7215 - val_accuracy: 0.4659\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6724 - accuracy: 0.5663 - val_loss: 0.7206 - val_accuracy: 0.4719\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6720 - accuracy: 0.5683 - val_loss: 0.7213 - val_accuracy: 0.4699\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6719 - accuracy: 0.5743 - val_loss: 0.7220 - val_accuracy: 0.4699\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6720 - accuracy: 0.5723 - val_loss: 0.7224 - val_accuracy: 0.4699\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6714 - accuracy: 0.5723 - val_loss: 0.7219 - val_accuracy: 0.4719\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6715 - accuracy: 0.5703 - val_loss: 0.7218 - val_accuracy: 0.4699\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6715 - accuracy: 0.5723 - val_loss: 0.7220 - val_accuracy: 0.4719\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6715 - accuracy: 0.5703 - val_loss: 0.7217 - val_accuracy: 0.4719\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6710 - accuracy: 0.5683 - val_loss: 0.7224 - val_accuracy: 0.4739\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6710 - accuracy: 0.5703 - val_loss: 0.7232 - val_accuracy: 0.4699\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6710 - accuracy: 0.5703 - val_loss: 0.7230 - val_accuracy: 0.4739\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6714 - accuracy: 0.5703 - val_loss: 0.7222 - val_accuracy: 0.4659\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6711 - accuracy: 0.5723 - val_loss: 0.7238 - val_accuracy: 0.4719\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6709 - accuracy: 0.5743 - val_loss: 0.7226 - val_accuracy: 0.4739\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6706 - accuracy: 0.5723 - val_loss: 0.7238 - val_accuracy: 0.4719\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6706 - accuracy: 0.5743 - val_loss: 0.7233 - val_accuracy: 0.4699\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6702 - accuracy: 0.5743 - val_loss: 0.7244 - val_accuracy: 0.4679\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6703 - accuracy: 0.5703 - val_loss: 0.7228 - val_accuracy: 0.4759\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6702 - accuracy: 0.5723 - val_loss: 0.7240 - val_accuracy: 0.4739\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6701 - accuracy: 0.5763 - val_loss: 0.7236 - val_accuracy: 0.4679\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6701 - accuracy: 0.5743 - val_loss: 0.7245 - val_accuracy: 0.4739\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6699 - accuracy: 0.5683 - val_loss: 0.7233 - val_accuracy: 0.4719\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6699 - accuracy: 0.5723 - val_loss: 0.7240 - val_accuracy: 0.4739\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6696 - accuracy: 0.5763 - val_loss: 0.7238 - val_accuracy: 0.4699\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6697 - accuracy: 0.5703 - val_loss: 0.7240 - val_accuracy: 0.4699\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6695 - accuracy: 0.5763 - val_loss: 0.7240 - val_accuracy: 0.4699\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6692 - accuracy: 0.5783 - val_loss: 0.7242 - val_accuracy: 0.4679\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6694 - accuracy: 0.5763 - val_loss: 0.7236 - val_accuracy: 0.4659\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6691 - accuracy: 0.5843 - val_loss: 0.7253 - val_accuracy: 0.4679\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6692 - accuracy: 0.5783 - val_loss: 0.7261 - val_accuracy: 0.4699\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6690 - accuracy: 0.5763 - val_loss: 0.7256 - val_accuracy: 0.4679\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6688 - accuracy: 0.5783 - val_loss: 0.7263 - val_accuracy: 0.4659\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6691 - accuracy: 0.5803 - val_loss: 0.7251 - val_accuracy: 0.4639\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6688 - accuracy: 0.5783 - val_loss: 0.7265 - val_accuracy: 0.4679\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6686 - accuracy: 0.5763 - val_loss: 0.7258 - val_accuracy: 0.4679\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6685 - accuracy: 0.5783 - val_loss: 0.7254 - val_accuracy: 0.4699\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6685 - accuracy: 0.5783 - val_loss: 0.7249 - val_accuracy: 0.4759\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6684 - accuracy: 0.5763 - val_loss: 0.7263 - val_accuracy: 0.4719\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6682 - accuracy: 0.5783 - val_loss: 0.7265 - val_accuracy: 0.4699\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6680 - accuracy: 0.5763 - val_loss: 0.7263 - val_accuracy: 0.4679\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6683 - accuracy: 0.5783 - val_loss: 0.7271 - val_accuracy: 0.4679\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6681 - accuracy: 0.5723 - val_loss: 0.7273 - val_accuracy: 0.4699\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6679 - accuracy: 0.5763 - val_loss: 0.7267 - val_accuracy: 0.4679\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6678 - accuracy: 0.5783 - val_loss: 0.7261 - val_accuracy: 0.4739\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6677 - accuracy: 0.5763 - val_loss: 0.7265 - val_accuracy: 0.4719\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6676 - accuracy: 0.5803 - val_loss: 0.7271 - val_accuracy: 0.4719\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6674 - accuracy: 0.5763 - val_loss: 0.7272 - val_accuracy: 0.4719\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6676 - accuracy: 0.5783 - val_loss: 0.7272 - val_accuracy: 0.4719\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6676 - accuracy: 0.5763 - val_loss: 0.7272 - val_accuracy: 0.4699\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6672 - accuracy: 0.5763 - val_loss: 0.7278 - val_accuracy: 0.4719\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6673 - accuracy: 0.5763 - val_loss: 0.7273 - val_accuracy: 0.4719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6671 - accuracy: 0.5743 - val_loss: 0.7270 - val_accuracy: 0.4719\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6669 - accuracy: 0.5763 - val_loss: 0.7282 - val_accuracy: 0.4719\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6668 - accuracy: 0.5783 - val_loss: 0.7280 - val_accuracy: 0.4719\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6667 - accuracy: 0.5783 - val_loss: 0.7278 - val_accuracy: 0.4759\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6666 - accuracy: 0.5763 - val_loss: 0.7282 - val_accuracy: 0.4779\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6668 - accuracy: 0.5763 - val_loss: 0.7278 - val_accuracy: 0.4799\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6666 - accuracy: 0.5803 - val_loss: 0.7301 - val_accuracy: 0.4679\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6667 - accuracy: 0.5783 - val_loss: 0.7289 - val_accuracy: 0.4739\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6664 - accuracy: 0.5763 - val_loss: 0.7306 - val_accuracy: 0.4679\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6661 - accuracy: 0.5743 - val_loss: 0.7284 - val_accuracy: 0.4759\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6662 - accuracy: 0.5723 - val_loss: 0.7290 - val_accuracy: 0.4779\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6663 - accuracy: 0.5763 - val_loss: 0.7278 - val_accuracy: 0.4779\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6659 - accuracy: 0.5783 - val_loss: 0.7275 - val_accuracy: 0.4739\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6659 - accuracy: 0.5783 - val_loss: 0.7278 - val_accuracy: 0.4739\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6657 - accuracy: 0.5783 - val_loss: 0.7289 - val_accuracy: 0.4719\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6662 - accuracy: 0.5743 - val_loss: 0.7311 - val_accuracy: 0.4699\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6658 - accuracy: 0.5783 - val_loss: 0.7296 - val_accuracy: 0.4779\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6664 - accuracy: 0.5783 - val_loss: 0.7290 - val_accuracy: 0.4759\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6656 - accuracy: 0.5763 - val_loss: 0.7311 - val_accuracy: 0.4719\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6655 - accuracy: 0.5743 - val_loss: 0.7292 - val_accuracy: 0.4719\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6652 - accuracy: 0.5763 - val_loss: 0.7309 - val_accuracy: 0.4719\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6653 - accuracy: 0.5763 - val_loss: 0.7302 - val_accuracy: 0.4719\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6652 - accuracy: 0.5763 - val_loss: 0.7304 - val_accuracy: 0.4739\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6649 - accuracy: 0.5783 - val_loss: 0.7299 - val_accuracy: 0.4739\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6654 - accuracy: 0.5783 - val_loss: 0.7303 - val_accuracy: 0.4819\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6661 - accuracy: 0.5683 - val_loss: 0.7330 - val_accuracy: 0.4679\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6655 - accuracy: 0.5783 - val_loss: 0.7289 - val_accuracy: 0.4739\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6647 - accuracy: 0.5763 - val_loss: 0.7299 - val_accuracy: 0.4719\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6645 - accuracy: 0.5763 - val_loss: 0.7304 - val_accuracy: 0.4719\n",
      "Epoch 315/500\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6589 - accuracy: 0.5938"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mModel(inputs\u001b[38;5;241m=\u001b[39mX, outputs\u001b[38;5;241m=\u001b[39mprediction)\n\u001b[1;32m     61\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mopt, loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 62\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_x_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_y_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_x_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2452\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2449\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m-> 2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mgraph_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2683\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2679\u001b[0m   cache_key, cache_key_deletion_observer \u001b[38;5;241m=\u001b[39m function_context\u001b[38;5;241m.\u001b[39mmake_cache_key(\n\u001b[1;32m   2680\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflat_input_signature)\n\u001b[1;32m   2682\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2683\u001b[0m   \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2685\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2686\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments supplied to `defun`-generated functions must be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2687\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhashable.  Original error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/core/function/polymorphism/function_cache.py:76\u001b[0m, in \u001b[0;36mFunctionCacheKey.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m---> 76\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_signature\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/core/function/trace_type/default_types.py:136\u001b[0m, in \u001b[0;36mOrderedCollection.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 136\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/core/function/trace_type/default_types.py:136\u001b[0m, in \u001b[0;36mOrderedCollection.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 136\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/core/function/trace_type/default_types.py:335\u001b[0m, in \u001b[0;36mReference.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 335\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/type_spec.py:447\u001b[0m, in \u001b[0;36mTypeSpec.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 447\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_cmp_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/tensor_spec.py:76\u001b[0m, in \u001b[0;36mDenseSpec.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 76\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filename = 'npr_lin_100.csv'\n",
    "filename1 = 'npr_circ_100.csv'\n",
    "\n",
    "# train the LSTM\n",
    "\n",
    "batch_x_train = []\n",
    "batch_y_train = []\n",
    "\n",
    "df_1 = pd.read_csv('np_lin_100.csv')\n",
    "#df_1 = df_1.drop('101', axis=1)\n",
    "#lst_0 = [0]*100\n",
    "#lst_1 = [1]*100\n",
    "\n",
    "for index, rows in df_1.iterrows():\n",
    "    batch_y_train.append(0)\n",
    "    list_1 = rows\n",
    "    batch_x_train.append(list_1)\n",
    "    \n",
    "df_2 = pd.read_csv('np_circ_100.csv')            \n",
    "\n",
    "for index, rows in df_2.iterrows():\n",
    "    batch_y_train.append(1)\n",
    "    list_2 = rows\n",
    "    batch_x_train.append(list_2)\n",
    "\n",
    "\n",
    "#batch_x_train = tf.keras.utils.to_categorical(batch_x_train) #turn this into a tensor not categorical\n",
    "batch_x_train = tf.convert_to_tensor(batch_x_train, dtype=tf.float32) \n",
    "#batch_x_train = np.asarray(batch_x_train, dtype=object)\n",
    "batch_y_train = tf.keras.utils.to_categorical(batch_y_train, num_classes=2)\n",
    "#batch_y_train = tf.convert_to_tensor(batch_y_train)\n",
    "\n",
    "batch_y_test = batch_y_train\n",
    "\n",
    "batch_x_test = []\n",
    "\n",
    "df_3 = pd.read_csv('np_t_lin_100.csv')\n",
    "\n",
    "for index, rows in df_3.iterrows():\n",
    "    list_3 = rows\n",
    "    batch_x_test.append(list_3)\n",
    "    \n",
    "df_4 = pd.read_csv('np_t_circ_100.csv')\n",
    "for index, rows in df_4.iterrows():\n",
    "    list_4 = rows\n",
    "    batch_x_test.append(list_4)\n",
    "\n",
    "batch_x_test = tf.convert_to_tensor(batch_x_test, dtype=tf.float32)\n",
    "\n",
    "lr = 1e-3\n",
    "num_nodes = 4\n",
    "num_classes = 2\n",
    "num_epochs = 500\n",
    "timesteps = 1\n",
    "tf.keras.backend.clear_session()\n",
    "X = tf.keras.Input(name='X', shape=[batch_x_train.shape[1],1], dtype=tf.dtypes.float32)\n",
    "lstm_output = tf.keras.layers.LSTM(num_nodes)(X)\n",
    "prediction = tf.keras.layers.Dense(num_classes, activation=tf.keras.activations.softmax)(lstm_output)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model = keras.Model(inputs=X, outputs=prediction)\n",
    "model.compile(optimizer=opt, loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "model.fit(x=batch_x_train, y=batch_y_train, epochs=num_epochs, validation_data=(batch_x_test, batch_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e749e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x_test = []\n",
    "\n",
    "df_3 = pd.read_csv('np_t_lin_100.csv')\n",
    "\n",
    "for index, rows in df_3.iterrows():\n",
    "    list_3 = rows\n",
    "    batch_x_test.append(list_3)\n",
    "    \n",
    "df_4 = pd.read_csv('np_t_circ_100.csv')\n",
    "for index, rows in df_4.iterrows():\n",
    "    list_4 = rows\n",
    "    batch_x_test.append(list_4)\n",
    "    \n",
    "batch_x_test = tf.convert_to_tensor(batch_x_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c94795c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step\n",
      "tf.Tensor(\n",
      "[[143 106]\n",
      " [ 84 165]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "y_prediction = model.predict(batch_x_test)\n",
    "y_prediction = np.argmax (y_prediction, axis =1)\n",
    "y_test = np.argmax(batch_y_train, axis=1)\n",
    "result = tf.math.confusion_matrix(y_test, y_prediction, num_classes=2, weights=None)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "509f20bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0\n",
      " 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1\n",
      " 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 1\n",
      " 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1\n",
      " 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1\n",
      " 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1\n",
      " 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0\n",
      " 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0\n",
      " 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0\n",
      " 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 1 0\n",
      " 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 0 0\n",
      " 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 0 1\n",
      " 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1\n",
      " 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "with np.printoptions(threshold=np.inf):\n",
    "    print(y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a38e242d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [\"linear\", \"circular\"]\n",
    "cm = confusion_matrix(y_test, y_prediction)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb91b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x_train_2 = []\n",
    "batch_y_train_2 = []\n",
    "\n",
    "df_3 = pd.read_csv('big_boi_s_1.csv', nrows=498)\n",
    "\n",
    "for index, rows in df_3.iterrows():\n",
    "    list_3 = rows\n",
    "    batch_x_train_2.append(list_3)\n",
    "    \n",
    "#batch_x_train = tf.keras.utils.to_categorical(batch_x_train) #turn this into a tensor not categorical\n",
    "batch_x_train_2 = tf.convert_to_tensor(batch_x_train_2, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5240dce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step\n",
      "tf.Tensor(\n",
      "[[141 108]\n",
      " [163  86]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "y_prediction = model.predict(batch_x_train_2)\n",
    "y_prediction = np.argmax (y_prediction, axis =1)\n",
    "y_test = np.argmax(batch_y_train, axis=1)\n",
    "result = tf.math.confusion_matrix(y_test, y_prediction, num_classes=2, weights=None)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b697ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = y_prediction\n",
    "res2 = res1.reshape(1,498)\n",
    "results = np.concatenate((batch_x_train_2, res2.T), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f680bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"big_boi_1_t.csv\", results,\n",
    "              delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d08f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('NP_LSTM.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a93284",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('NP_LSTM.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c799a02a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
